# -*- coding: utf-8 -*-
"""DL-1(Section 2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kxuyl3-KBblHUFmAWqvJbGDryWk8AAxG
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

!pip install -U datasets

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers
# !pip install rouge_score

from datasets import load_metric
import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/code cycle /wikiHow.csv")
df.head()

print(df.shape)
df = df.dropna()
print(df.shape)

print(df.shape)
df = df.drop_duplicates()
print(df.shape)

df['length'] = df.paragraph.map(lambda x:len(x.split(" ")))

numOfWords = df.length
from matplotlib import pyplot as plt

fig = plt.figure(figsize =(5, 3))
plt.hist(numOfWords.to_numpy(), bins = [0,50,100,200,300,500,1000])
plt.title("Word count distribution")
plt.show()

tempDf = df[df.length <= 200]
tempDf.shape

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("allenai/led-base-16384")

max_input_length = 1024
max_output_length = 64
batch_size = 16

def process_data_to_model_inputs(batch):
    inputs = tokenizer(
        batch["paragraph"],
        padding="max_length",
        truncation=True,
        max_length=max_input_length,
    )
    outputs = tokenizer(
        batch["heading"],
        padding="max_length",
        truncation=True,
        max_length=max_output_length,
    )

    batch["input_ids"] = inputs.input_ids
    batch["attention_mask"] = inputs.attention_mask

    batch["global_attention_mask"] = len(batch["input_ids"]) * [
        [0 for _ in range(len(batch["input_ids"][0]))]
    ]

    batch["global_attention_mask"][0][0] = 1
    batch["labels"] = outputs.input_ids

    batch["labels"] = [
        [-100 if token == tokenizer.pad_token_id else token for token in labels]
        for labels in batch["labels"]
    ]

    return batch

import numpy as np
train,validate,test = np.split(tempDf.sample(frac=1, random_state=42), [int(.6*len(tempDf)), int(.7*len(tempDf))])
print(train.shape)
print(validate.shape)
print(test.shape)

validate = validate[:20]
validate.shape

from datasets import Dataset
train_dataset = Dataset.from_pandas(train)
val_dataset = Dataset.from_pandas(validate)

train_dataset = train_dataset.map(
    process_data_to_model_inputs,
    batched=True,
    batch_size=batch_size,
    remove_columns=["title","heading","paragraph","length","__index_level_0__"],
)

val_dataset = val_dataset.map(
    process_data_to_model_inputs,
    batched=True,
    batch_size=batch_size,
    remove_columns=["title","heading","paragraph","length","__index_level_0__"]
)

train_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "global_attention_mask", "labels"],
)
val_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "global_attention_mask", "labels"],
)

from transformers import AutoModelForSeq2SeqLM
led = AutoModelForSeq2SeqLM.from_pretrained("allenai/led-base-16384", gradient_checkpointing=True, use_cache=False)
led.config.num_beams = 2
led.config.max_length = 64
led.config.min_length = 2
led.config.length_penalty = 2.0
led.config.early_stopping = True
led.config.no_repeat_ngram_size = 3
rouge = load_metric("rouge")

def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    rouge_output = rouge.compute(
        predictions=pred_str, references=label_str, rouge_types=["rouge2"]
    )

    return {
        "rouge2_precision": round(rouge_output.precision, 4),
        "rouge2_recall": round(rouge_output.recall, 4),
        "rouge2_fmeasure": round(rouge_output.fmeasure, 4),
    }

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import transformers
transformers.logging.set_verbosity_info()

training_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    eval_strategy="steps",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    output_dir="./",
    logging_steps=5,
    eval_steps=10,
    save_total_limit=2,
    gradient_accumulation_steps=4,
    num_train_epochs=10
)

trainer = Seq2SeqTrainer(
    model=led,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

import pandas as pd
sample_paragraph = """The reason why I loved the top-down culture at Apple is that important decisions
are taken faster. Having an expert giving you green light or not keeps the
momentum. How many times in a bottom-up culture do we spend weeks and
weeks, sometimes even months, trying to get alignment with +10 people, because
every single person needs to agree with the point of view? It is exhausting. So
again, my experience is that having that one leader to look up to to help guide
decisions is time-saving, it helps us focus on the design craft, instead of project
”management."""
data = [sample_paragraph]
df = pd.DataFrame(data, columns=['paragraph'])
df['paragraph'][0]
from datasets import Dataset
df_test = Dataset.from_pandas(df)
df_test

from datasets import load_metric
import torch

from datasets import load_dataset, load_metric
from transformers import LEDTokenizer, LEDForConditionalGeneration

# load tokenizer
tokenizer = LEDTokenizer.from_pretrained("/content/checkpoint-130")
model = LEDForConditionalGeneration.from_pretrained("/content/checkpoint-130").to("cuda").half()

def generate_answer(batch):
    inputs_dict = tokenizer(batch["paragraph"], padding="max_length", max_length=512, return_tensors="pt", truncation=True)
    input_ids = inputs_dict.input_ids.to("cuda")
    attention_mask = inputs_dict.attention_mask.to("cuda")
    global_attention_mask = torch.zeros_like(attention_mask)
    # put global attention on token
    # global_attention_mask[:, 0] = 1

    predicted_abstract_ids = model.generate(input_ids, attention_mask=attention_mask, global_attention_mask=global_attention_mask)
    batch["generated_heading"] = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)
    return batch

result = df_test.map(generate_answer, batched=True, batch_size=2)

result["generated_heading"]